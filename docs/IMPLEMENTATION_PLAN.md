# MCP Trading Platform Build Plan

This document captures the implementation approach for aligning the current repository with the full requirements outlined in:

- `docs/MCP_PumpFun_Server_spec.txt`
- `docs/MCP_Trader_Monitoring_Spec.txt`

It highlights the new services, shared infrastructure, and milestones that will be delivered across the Next.js monitoring application and the standalone MCP server.

## 1. Shared Infrastructure

| Concern | Implementation |
| --- | --- |
| **Database** | Prisma models expanded with `TokenStat`, `AgentWatchlist`, `AgentRule`, `AgentSignal`, and `CacheEntry` to persist aggregator snapshots, rules, watchlist, generated events, and cached responses. Existing token/trade tables remain the source of truth for historical data. |
| **Caching** | In-memory LRU cache backed by the `cache_kv` table for durability across restarts. All outbound Pump.fun requests pass through a cache/rate-limit wrapper that persists TTL metadata. |
| **Rate Limiting** | Token-bucket implementation configurable per host, enforcing the per-endpoint cadence defined in the specs. |
| **Config & Secrets** | Consolidated `config` module providing typed access to environment variables (DB, Redis, Pump.fun hosts, RPC URLs, etc.). |
| **Logging & Metrics** | Pino-based JSON logger with request-scoped correlation IDs. OpenTelemetry hooks added as part of future work (stubbed for now). |

## 2. MCP Server (Standalone)

### Process Layout

```
mcp-server/
  ├─ cmd/
  │   └─ mcp-server.ts       (bootstrap + graceful shutdown)
  ├─ core/
  │   ├─ ws/
  │   │   └─ ingestor.ts     (Pump.fun WebSocket listener, dedupe, persistence)
  │   ├─ agg/
  │   │   └─ aggregator.ts   (rolling windows, EMA/ATR, impact estimates)
  │   └─ rules/
  │       └─ engine.ts       (threshold evaluation, AgentSignal writes)
  ├─ services/
  │   ├─ cache.ts            (LRU + DB-backed cache)
  │   ├─ http.ts             (fetch wrapper, rate limits, retries)
  │   ├─ repo.ts             (typed Prisma accessors)
  │   └─ scheduler.ts        (polling cadence for REST backfills)
  └─ tools/
      ├─ discoverUniverse.ts
      ├─ tokenStats.ts
      ├─ recentTrades.ts
      ├─ holdersSnapshot.ts
      ├─ marketActivity.ts
      ├─ candles.ts
      ├─ solPrice.ts
      ├─ poolState.ts
      ├─ watchlist.ts
      ├─ portfolio.ts
      ├─ rulesEngine.ts
      └─ execTrade.ts
```

### Key Flows

1. **WebSocket Ingestor** normalizes `tradeCreated` frames, dedupes on `(signature, tx_index)`, and writes to `Trade`.
2. **Aggregator** maintains per-mint ring buffers for 30s/1m/5m windows, updating `TokenStat` rows twice per second with derived metrics (EMA, ATR, impact estimates).
3. **REST Fetchers** run on the cadence defined in the spec, storing normalized snapshots (candles, holders, market-activity, ATH) and updating caches.
4. **Rules Engine** evaluates expressions stored in `AgentRule`, producing `AgentSignal` documents that double as EVENT frames for the MCP transport layer.
5. **MCP Tools** are implemented as stateless handlers that rely on the repository/cache layers. All responses include `asOf` timestamps, and errors surface structured codes suitable for the agent.

## 3. Monitoring & Control App (Next.js)

### Back-End Enhancements

- `/api` route handlers expanded to mirror the monitoring spec (portfolio, positions, trades, orders, tokens, agent events, chat, alert config).
- WebSocket hub implemented at `/api/stream` (Socket.IO) routing server-side events to subscribed clients.
- Background ingestion worker (Next.js route handler + edge-safe scheduler) consumes Redis Pub/Sub channels emitting the same normalized events generated by the MCP ingestion pipeline.
- Queue-backed order orchestration endpoint (`POST /api/orders`) enqueues intents for either MCP tool execution or a future Execution microservice.

### Front-End Enhancements (App Router)

- Reworked dashboard (`/`) with live cards for equity, PnL, SOL price, active orders, alerts, and execution timeline.
- Dedicated pages for `/positions`, `/trades`, `/orders`, `/tokens`, `/agent`, `/chat`, `/alerts`, and `/settings` with modular components (charts, tables, panels) that subscribe to the WebSocket stream.
- Chat window enforces ≤200k-token context budget with automatic eviction and “inject snapshot” actions.

### Event & Chat Streaming

- Normalized events (`portfolio:update`, `order:update`, `trade:new`, `agent:event`, `token:update`, `chat:new`) persisted and published through Redis. Clients keep in sync via the WebSocket hub.
- Chat management APIs enforce the rolling context rules and allow operator commands to trigger structured tool messages.

## 4. Milestones

1. **Infrastructure Foundation** — database migrations, shared config/logger/cache, rate-limited HTTP client.
2. **MCP Server Core** — WebSocket ingest, aggregator, REST polling, rules engine, MCP transport with the complete tool set.
3. **Monitoring Backend** — API routes, WebSocket hub, ingestion workers bridging MCP data, order/chat handling.
4. **Monitoring Frontend** — dashboards, detailed pages, chat/context UI, alert/risk controls.
5. **Testing & Ops** — unit/integration tests, linting, dockerization, observability scaffolding.

## 5. Outstanding Questions

1. Should trade execution remain proxied through MCP tools or be performed directly by the monitoring app?
2. Which default filters seed the discovery/watchlist pipeline (mirroring `/coins/for-you`)?
3. What environments (dev/stage/prod) are expected for automated deployments?

Each milestone will land in dedicated PR-sized commits to keep review surfaces manageable. The remainder of this implementation will proceed stepwise alongside the plan tracked in the CLI.
